# ğŸ¤– Gesture-Controlled Mobile Robot (ROS 2)

This repository documents my work on a **gesture-controlled autonomous mobile robot** developed using **ROS 2**, bridging computer vision, humanâ€“robot interaction, and mobile robotics simulation.

Building on my foundation in URDF, Xacro, Gazebo, and RViz, this project focuses on controlling a mobile robot using **hand gestures detected via MediaPipe**, translating human motion into real-time robot navigation inside a simulated environment.

---

## ğŸ¯ Objective

To design and implement a gesture-based control system for a mobile robot by integrating:

* ROS 2 mobile robot simulation (Gazebo)
* Hand gesture recognition using MediaPipe
* Python-based gesture processing node
* Differential drive motion control
* Real-time command publishing in ROS 2

This project explores **intuitive humanâ€“robot interaction**, where the robot responds naturally to human hand movements rather than traditional input devices.

---

## ğŸ§  What I Learned

| Area               | Knowledge Gained                                     |
| ------------------ | ---------------------------------------------------- |
| ROS 2 Architecture | Nodes, topics, launch files, workspace management    |
| Computer Vision    | Hand landmark detection using MediaPipe              |
| Gesture Mapping    | Translating gestures into velocity commands          |
| Mobile Robotics    | Differential drive motion control                    |
| Simulation         | Gazebo-based robot behavior testing                  |
| Debugging          | ROS graph issues, timing, and coordinate consistency |

This project strengthened my understanding of **human-in-the-loop robotic control systems**.

---

## ğŸš€ Milestones

* Created a mobile robot simulation in Gazebo
* Integrated MediaPipe for real-time hand tracking
* Developed a gesture control ROS 2 node
* Achieved smooth gesture-based robot motion
* Validated control logic in simulation

---

## ğŸ“Œ Significance

This project represents a step toward **natural and intuitive robot control**.

Instead of keyboards or joysticks, the robot responds directly to human gestures â€” a core idea behind collaborative robots and future autonomous systems.

It also reinforced the importance of synchronizing perception, decision-making, and motion in robotic systems.

---

## ğŸ”® Next Steps

Planned improvements include:

* Gesture smoothing and filtering
* Closed-loop control using simulated encoders
* Camera-based obstacle awareness
* Integration with ROS 2 Navigation Stack
* Transition from simulation to real hardware

---

## ğŸ™ Acknowledgment

Special thanks to **Articulated Robotics â€” Josh Newans**.

His tutorials on ROS 2, URDF, Gazebo, and mobile robotics were instrumental in building a strong conceptual foundation that made this project possible.

---

## ğŸ‘¤ Author

**Dilip Kumar S**
Robotics Developer

ğŸ“§ Email: [letsmaildilip@gmail.com](mailto:letsmaildilip@gmail.com)
ğŸ”— GitHub: [https://github.com/dilip-2006](https://github.com/dilip-2006)

---

If this project helps or inspires your robotics journey, a â­ on the repository would mean a lot!
